{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_tf_text_generation_rnn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNWPI1INznnQ5/46WH+/XBy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kozzlov/nlp_tf_text_generation_rnn/blob/main/nlp_tf_text_generation_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIy6dBnTnQCj",
        "outputId": "da9b1cd1-f325-4f2b-b92b-5ae8ea59f15e"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.layers.experimental import preprocessing\r\n",
        "import numpy as np \r\n",
        "import os\r\n",
        "import time\r\n",
        "\r\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\r\n",
        "\r\n",
        "#read, then decode for py2 compat\r\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\r\n",
        "#length of the text is the number of characters in it\r\n",
        "print('Length of the: {} characters'.format(len(text)))\r\n",
        "#unique characters\r\n",
        "vocab = sorted(set(text))\r\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "Length of the: 1115394 characters\n",
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qP91pH__mLb"
      },
      "source": [
        "#text processing\r\n",
        "#vectorising the text\r\n",
        "example_texts = ['abcdefg', 'xyz']\r\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\r\n",
        "chars\r\n",
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab))\r\n",
        "#it converts from the character ids, padding with 0\r\n",
        "ids = ids_from_chars(chars)\r\n",
        "ids\r\n",
        "#stringlookup allows to recover human-readable text\r\n",
        "chars_from_ids= tf.keras.layers.experimental.preprocessing.StringLookup(\r\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True)\r\n",
        "chars = chars_from_ids(ids)\r\n",
        "chars\r\n",
        "#tf.strings.reduce_join allows to join the characters into strings\r\n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()\r\n",
        "def text_from_ids(ids):\r\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uq0lsTibQzPr",
        "outputId": "66f53929-f723-4ee7-a4bc-0ebdf8c087e2"
      },
      "source": [
        "#the prediction task\r\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\r\n",
        "all_ids\r\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\r\n",
        "for ids in ids_dataset.take(5):\r\n",
        "  print(chars_from_ids(ids).numpy().decode('utf-8'))\r\n",
        "\r\n",
        "seq_length = 100\r\n",
        "examples_per_epoch = len(text)//(seq_length+1)\r\n",
        "\r\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\r\n",
        "# for seq in sequences.take(1):\r\n",
        "#   print(chars_from_ids(seq))\r\n",
        "# for seq in sequences.take(5):\r\n",
        "#   print(text_from_ids(seq).numpy())\r\n",
        "\r\n",
        "def split_input_target(sequences):\r\n",
        "  input_text = sequences[:-1]\r\n",
        "  target_text= sequences[1:]\r\n",
        "  return input_text, target_text\r\n",
        "# split_input_target(list(\"Tensorflow\"))\r\n",
        "\r\n",
        "dataset = sequences.map(split_input_target)\r\n",
        "\r\n",
        "for input_example, target_example in dataset.take(1):\r\n",
        "  print(\"Input :\", text_from_ids(input_example).numpy())\r\n",
        "  print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQjQQ0Tmdcik",
        "outputId": "f8cec024-097b-4776-c3ea-55b5b09bd831"
      },
      "source": [
        "#create training batches\r\n",
        "BATCH_SIZE = 64 \r\n",
        "BUFFER_SIZE = 10000\r\n",
        "#buffersize to shuffle the dataset\r\n",
        "#(TF data is designed to work with possibly infinite sequences,\r\n",
        "#so it doesn't attempt to shuffle the entire sequence in memory. Instead,\r\n",
        "#it maintains a buffer in which it shuffles elements).\r\n",
        "\r\n",
        "dataset = (\r\n",
        "    dataset\r\n",
        "    .shuffle(BUFFER_SIZE)\r\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\r\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\r\n",
        "\r\n",
        "dataset"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJdpAp4iklCn"
      },
      "source": [
        "#build the model \r\n",
        "#length of the vocabulary in chain \r\n",
        "vocab_size = len(vocab)\r\n",
        "#the embedding dimension\r\n",
        "embedding_dim = 256\r\n",
        "#number of rnn units \r\n",
        "rnn_units = 1024\r\n",
        "\r\n",
        "class Model(tf.keras.Model):\r\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\r\n",
        "    super().__init__(self)\r\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n",
        "    self.gru = tf.keras.layers.GRU(\r\n",
        "        rnn_units,\r\n",
        "        return_sequences=True,\r\n",
        "        return_state=True)\r\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\r\n",
        "\r\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\r\n",
        "    x = inputs\r\n",
        "    x = self.embedding(x, training=training)\r\n",
        "    if states is None:\r\n",
        "      states = self.gru.get_initial_state(x)\r\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\r\n",
        "    x = self.dense(x, training=training)\r\n",
        "\r\n",
        "    if return_state:\r\n",
        "      return x, states\r\n",
        "    else:\r\n",
        "      return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVwtorv2X0bF"
      },
      "source": [
        "model = Model(\r\n",
        "    #vocabulary size must match the 'StringLookup layers\r\n",
        "    vocab_size = len(ids_from_chars.get_vocabulary()),\r\n",
        "    embedding_dim = embedding_dim,\r\n",
        "    rnn_units = rnn_units)\r\n",
        "\r\n",
        "# For each character the model looks up the embedding,\r\n",
        "# runs the GRU one timestep with the embedding as input,\r\n",
        "# and applies the dense layer to generate logits predicting \r\n",
        "# the log-likelihood of the next character:\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euJbpH2-ZqEE",
        "outputId": "30c2bc7a-bb78-4e4e-a0c0-e9d8825a96d5"
      },
      "source": [
        "#model summary details\r\n",
        "#checking the shape of an output\r\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\r\n",
        "  example_batch_predictions = model(input_example_batch)\r\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 67) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUOII16xpRak",
        "outputId": "38601fe5-902e-479d-b847-0ee7ceb2895c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  17152     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  68675     \n",
            "=================================================================\n",
            "Total params: 4,024,131\n",
            "Trainable params: 4,024,131\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FHBdJeRpvQf",
        "outputId": "2ff31ecc-a7dc-4945-cc76-f4cee89c525a"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\r\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\r\n",
        "#first example in the batch \r\n",
        "sampled_indices\r\n",
        "#decoding the text predicted by the untrained model\r\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\r\n",
        "print()\r\n",
        "print(\"Next char predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b\" I fly to 'scape their hands?\\nAh, tutor, look where bloody Clifford comes!\\n\\nCLIFFORD:\\nChaplain, away\"\n",
            "\n",
            "Next char predictions:\n",
            " b\"$f -J.loGD'NO::Ta& ugUzSnPQqm!v'-pnHBezDrPK!zj',KHu!UBEq?Bsl3ajI?[UNK]gIvoNs?Lu:$ kFf\\njIx,aOrg;aIwLQ\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKW_dMqWq-df",
        "outputId": "fe989881-d5c3-4cf6-bc42-3676c4212382"
      },
      "source": [
        "#train the model\r\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\r\n",
        "mean_loss = example_batch_loss.numpy().mean()\r\n",
        "print(\"pred shape: \", example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\r\n",
        "print(\"mean loss:  \", mean_loss)\r\n",
        "#checking model's awareness in providing incorrect answers\r\n",
        "tf.exp(mean_loss).numpy()\r\n",
        "\r\n",
        "model.compile(optimizer='adam', loss=loss)\r\n",
        "\r\n",
        "#configuring if checkpoints were saved during training \r\n",
        "checkpoint_dir = './training_checkpoints'\r\n",
        "#naming the checkpoints file\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n",
        "\r\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n",
        "    filepath = checkpoint_prefix,\r\n",
        "    save_weights_only = True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pred shape:  (64, 100, 67) # (batch_size, sequence_length, vocab_size)\n",
            "mean loss:   4.206151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ddlmd3DnwiLs",
        "outputId": "695fa160-cd24-4749-a186-a31f305f3799"
      },
      "source": [
        "#execute training \r\n",
        "EPOCHS = 25 \r\n",
        "history = model.fit(dataset, \r\n",
        "                    epochs=EPOCHS,\r\n",
        "                    callbacks=[checkpoint_callback])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.7087\n",
            "Epoch 2/25\n",
            "172/172 [==============================] - 24s 134ms/step - loss: 0.6663\n",
            "Epoch 3/25\n",
            "172/172 [==============================] - 24s 134ms/step - loss: 0.6270\n",
            "Epoch 4/25\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.5927\n",
            "Epoch 5/25\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.5635\n",
            "Epoch 6/25\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.5389\n",
            "Epoch 7/25\n",
            "172/172 [==============================] - 24s 134ms/step - loss: 0.5195\n",
            "Epoch 8/25\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.5034\n",
            "Epoch 9/25\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.4900\n",
            "Epoch 10/25\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.4750\n",
            "Epoch 11/25\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.4639\n",
            "Epoch 12/25\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.4542\n",
            "Epoch 13/25\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.4497\n",
            "Epoch 14/25\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.4440\n",
            "Epoch 15/25\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.4417\n",
            "Epoch 16/25\n",
            "172/172 [==============================] - 24s 132ms/step - loss: 0.4322\n",
            "Epoch 17/25\n",
            "172/172 [==============================] - 24s 132ms/step - loss: 0.4245\n",
            "Epoch 18/25\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.4248\n",
            "Epoch 19/25\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.4219\n",
            "Epoch 20/25\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.4200\n",
            "Epoch 21/25\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.4221\n",
            "Epoch 22/25\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.4152\n",
            "Epoch 23/25\n",
            "172/172 [==============================] - 24s 134ms/step - loss: 0.4162\n",
            "Epoch 24/25\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.4156\n",
            "Epoch 25/25\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.4135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWAxAEbP5u_h"
      },
      "source": [
        "#generate text \r\n",
        "class Step(tf.keras.Model):\r\n",
        "  def __init__(self, model, chars_froom_ids, ids_from_chars, temperature=1.0):\r\n",
        "    super().__init__()\r\n",
        "    self.temperature = temperature\r\n",
        "    self.model = model\r\n",
        "    self.chars_from_ids = chars_from_ids\r\n",
        "    self.ids_from_chars = ids_from_chars\r\n",
        "  \r\n",
        "    #create a mask to prevent \"\" or \"[UNK]\" from being generated\r\n",
        "    skip_ids = self.ids_from_chars(['', '[UNK]'])[:, None]\r\n",
        "    sparse_mask = tf.SparseTensor(\r\n",
        "        # put -inf to each bad index\r\n",
        "        values=[-float('inf')]*len(skip_ids),\r\n",
        "        indices = skip_ids,\r\n",
        "        #match the shape to the vocabulary\r\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\r\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\r\n",
        "\r\n",
        "  @tf.function\r\n",
        "  def generate_one_step(self, inputs, states=None):\r\n",
        "    #convert strings to tokens ids\r\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\r\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\r\n",
        "    #running the model\r\n",
        "    #predicted_logist.shape is [batch, char, next_char_logits]\r\n",
        "    predicted_logits, states = self.model(inputs=input_ids,\r\n",
        "                                          states=states,\r\n",
        "                                          return_state=True)\r\n",
        "    #use only the last prediction \r\n",
        "    predicted_logits = predicted_logits[:, -1, :]\r\n",
        "    precicted_logits = predicted_logits/self.temperature\r\n",
        "    #apply the predictions mask: prevent \"\" or \"[UNK]\" form being generated\r\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\r\n",
        "    #sample the output logits to generate token ids\r\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\r\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=1)\r\n",
        "    #convert from token ids to characters\r\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\r\n",
        "    #return the characters and model state\r\n",
        "    return predicted_chars, states"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s90y4lGnHrh",
        "outputId": "650b4474-c997-4609-cd8e-d7f0e28161e1"
      },
      "source": [
        "one_step_model = Step(model, chars_from_ids, ids_from_chars)\r\n",
        "#try to generate text in a loop\r\n",
        "start = time.time()\r\n",
        "states = None\r\n",
        "next_char = tf.constant(['ROMEO:'])\r\n",
        "result = [next_char]\r\n",
        "\r\n",
        "for n in range(1000):\r\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\r\n",
        "  result.append(next_char)\r\n",
        "\r\n",
        "result = tf.strings.join(result)\r\n",
        "end = time.time()\r\n",
        "\r\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\r\n",
        "print(f\"\\nRun time: {end - start}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "Menerity, my meed hath gentle a vole.\n",
            "\n",
            "Nurse:\n",
            "My bushles will mark you this earth again,\n",
            "Not Mortageous Cautia? she must needs above;\n",
            "But execution, am I at their backs,\n",
            "Spuking him from thence, that this hard hate his brows:\n",
            "For never was my follying to the worse.\n",
            "Here, Awaking, that he here.\n",
            "\n",
            "DORCAS:\n",
            "If the law which way, though we will confess\n",
            "I then lure and long tribute's majesty\n",
            "To strive the kities of doom and neighbours.\n",
            "Now, nurse, go thy way, into me,\n",
            "And what you will, if she be, 'sint thou keep her\n",
            "Solicits, and with him on his breast, and joy\n",
            "The more envointed of his lady's heir!\n",
            "\n",
            "TRANIO:\n",
            "Fear me not.\n",
            "\n",
            "EXETER:\n",
            "No mother, boy.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Do you thanks.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "He followeds: defend us breathe against the house\n",
            "Ireld that be revenged on those I am,\n",
            "For her no deep desperate which you have\n",
            "To hear thee say and you my servants\n",
            "From our better to the blore. Although\n",
            "I do not like the loss of mine,\n",
            "Which burn in meaning, if thou art all us.\n",
            "\n",
            "ROMEO:\n",
            "Good art watch \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.362096309661865\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}